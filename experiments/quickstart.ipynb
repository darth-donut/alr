{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Quick-start\n",
    "\n",
    "This document helps you get up-and-running with `alr` immediately.\n",
    "It should give you a general idea of how to get started with\n",
    "this package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as torchdata\n",
    "import tqdm\n",
    "import sys\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "from alr import MCDropout\n",
    "from alr.acquisition import RandomAcquisition\n",
    "from alr.utils import stratified_partition\n",
    "from alr.data import DataManager, UnlabelledDataset\n",
    "from alr.data.datasets import Dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "data_loader_params = dict(pin_memory=True, num_workers=2, batch_size=32)\n",
    "device = torch.device('gpu:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly, we load and prepare our data.\n",
    "Note that we partitioned the training set into labelled and unlabelled sets\n",
    "using `stratified partition` which balances the number of classes in the training pool:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(20, 10000, 59980)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load training data\n",
    "train, test = Dataset.MNIST.get()\n",
    "pool, train = stratified_partition(train, Dataset.MNIST.about.n_class, 20)\n",
    "pool = UnlabelledDataset(pool)\n",
    "len(train), len(test), len(pool)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`MCDropout` lets us define a Bayesian NN. It provides an implementation\n",
    "for `stochastic_forward` which we will use in the next section for the\n",
    "acquisition function.\n",
    "\n",
    "> Notice the dropout layers have been changed to their `Persistent` versions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "MCDropout(\n  (base_model): Net(\n    (fc1): Linear(in_features=784, out_features=1024, bias=True)\n    (drop1): PersistentDropout(p=0.5, inplace=False)\n    (fc2): Linear(in_features=1024, out_features=2048, bias=True)\n    (drop2): PersistentDropout(p=0.5, inplace=False)\n    (fc3): Linear(in_features=2048, out_features=10, bias=True)\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a regular model and an optimiser.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 1024)\n",
    "        self.drop1 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(1024, 2048)\n",
    "        self.drop2 = nn.Dropout()\n",
    "        self.fc3 = nn.Linear(2048, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.drop1(self.fc1(x)))\n",
    "        x = F.relu(self.drop2(self.fc2(x)))\n",
    "        return self.fc3(x)\n",
    "\n",
    "model = MCDropout(Net(), forward=10).to(device)\n",
    "optimiser = torch.optim.Adam(model.parameters())\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can instantiate an acquisition function\n",
    "and an associated `DataManager` instance:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "ra = RandomAcquisition()\n",
    "dm = DataManager(train, pool, ra)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we define the usual training and evaluation loops:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "          dataloader: torchdata.DataLoader,\n",
    "          optimiser: torch.optim.Optimizer,\n",
    "          epochs: int = 50):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    tepochs = tqdm.trange(epochs, file=sys.stdout)\n",
    "    for _ in tepochs:\n",
    "        epoch_losses = []\n",
    "        for x, y in dataloader:\n",
    "            if device:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            epoch_losses.append(loss.item())\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        losses.append(np.mean(epoch_losses))\n",
    "        tepochs.set_postfix(loss=losses[-1])\n",
    "    return losses\n",
    "\n",
    "\n",
    "def evaluate(model: MCDropout,\n",
    "             dataloader: torchdata.DataLoader) -> float:\n",
    "    model.eval()\n",
    "    score = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            if device:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "            _, preds = torch.max(model.predict(x), dim=1)\n",
    "            score += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return score / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can put everything together:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquisition iteration 1 (10.00%), training size: 20\n",
      "100%|██████████| 10/10 [00:01<00:00,  6.93it/s, loss=0.0134]\n",
      "Accuracy = 0.5313\n",
      "=====\n",
      "Acquisition iteration 2 (20.00%), training size: 30\n",
      "100%|██████████| 10/10 [00:01<00:00,  7.17it/s, loss=0.027] \n",
      "Accuracy = 0.5548\n",
      "=====\n",
      "Acquisition iteration 3 (30.00%), training size: 40\n",
      "100%|██████████| 10/10 [00:01<00:00,  6.24it/s, loss=0.0332]\n",
      "Accuracy = 0.5606\n",
      "=====\n",
      "Acquisition iteration 4 (40.00%), training size: 50\n",
      "100%|██████████| 10/10 [00:01<00:00,  6.09it/s, loss=0.0664]\n",
      "Accuracy = 0.6117\n",
      "=====\n",
      "Acquisition iteration 5 (50.00%), training size: 60\n",
      "100%|██████████| 10/10 [00:01<00:00,  6.16it/s, loss=0.00393]\n",
      "Accuracy = 0.6412\n",
      "=====\n",
      "Acquisition iteration 6 (60.00%), training size: 70\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.42it/s, loss=0.0807]\n",
      "Accuracy = 0.6809\n",
      "=====\n",
      "Acquisition iteration 7 (70.00%), training size: 80\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.31it/s, loss=0.0062]\n",
      "Accuracy = 0.6931\n",
      "=====\n",
      "Acquisition iteration 8 (80.00%), training size: 90\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.12it/s, loss=0.00713]\n",
      "Accuracy = 0.6961\n",
      "=====\n",
      "Acquisition iteration 9 (90.00%), training size: 100\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.64it/s, loss=0.0389]\n",
      "Accuracy = 0.7122\n",
      "=====\n",
      "Acquisition iteration 10 (100.00%), training size: 110\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.63it/s, loss=0.0044]\n",
      "Accuracy = 0.7178\n",
      "=====\n"
     ]
    },
    {
     "data": {
      "text/plain": "{20: 0.5313,\n 30: 0.5548,\n 40: 0.5606,\n 50: 0.6117,\n 60: 0.6412,\n 70: 0.6809,\n 80: 0.6931,\n 90: 0.6961,\n 100: 0.7122,\n 110: 0.7178}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ITERS = 10\n",
    "EPOCHS = 10\n",
    "accs = {}\n",
    "\n",
    "# In each iteration, acquire 10 points at once and re-evaluate the model\n",
    "for i in range(ITERS):\n",
    "    print(f\"Acquisition iteration {i + 1} ({(i + 1) / ITERS:.2%}), \"\n",
    "          f\"training size: {dm.n_labelled}\")\n",
    "    model.reset_weights()\n",
    "    train(model, dataloader=torchdata.DataLoader(dm.labelled, **data_loader_params),\n",
    "          optimiser=optimiser, epochs=EPOCHS)\n",
    "    accs[dm.n_labelled] = evaluate(model, torchdata.DataLoader(test,\n",
    "                                                               **data_loader_params))\n",
    "    print(f\"Accuracy = {accs[dm.n_labelled]}\\n=====\")\n",
    "    dm.acquire(b=10)\n",
    "accs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}