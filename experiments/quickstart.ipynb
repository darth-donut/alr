{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Quick-start\n",
    "\n",
    "This document helps you get up-and-running with `alr` immediately.\n",
    "It should give you a general idea of how to get started with\n",
    "this package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as torchdata\n",
    "import tqdm\n",
    "import sys\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from sklearn.datasets import make_moons\n",
    "from torch import nn\n",
    "\n",
    "from alr import MCDropout, stratified_partition, DataManager\n",
    "from alr.acquisition import BALD\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "data_loader_params = dict(pin_memory=True, num_workers=2, batch_size=32)\n",
    "device = torch.device('gpu:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly, we prepare our data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([100, 2]), torch.Size([50, 2]))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load training data\n",
    "data = make_moons(n_samples=150)\n",
    "X_train, y_train = torch.as_tensor(data[0], dtype=torch.float32), torch.as_tensor(data[1])\n",
    "X_test, y_test = X_train[-50:], y_train[-50:]\n",
    "X_train, y_train = X_train[:-50], y_train[:-50]\n",
    "X_train.size(), X_test.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "and partition them into labelled and unlabelled sets using `stratified partition`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# partition data using stratified_partition\n",
    "X_train, y_train, X_pool, y_pool = stratified_partition(X_train, y_train, train_size=20)\n",
    "\n",
    "# create test DataLoader object\n",
    "test_dataset = torchdata.DataLoader(torchdata.TensorDataset(X_test, y_test),\n",
    "                                    **data_loader_params)\n",
    "len(test_dataset.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "50"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "`MCDropout` lets us define a Bayesian NN. It provides an implementation\n",
    "for `stochastic_forward` which we will use in the next section for the\n",
    "acquisition function.\n",
    "\n",
    "> Notice the dropout layers have been changed to their `Persistent` versions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "MCDropout(\n  (base_model): Net(\n    (fc1): Linear(in_features=2, out_features=128, bias=True)\n    (drop1): PersistentDropout(p=0.5, inplace=False)\n    (fc2): Linear(in_features=128, out_features=256, bias=True)\n    (drop2): PersistentDropout(p=0.5, inplace=False)\n    (fc3): Linear(in_features=256, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a regular model and an optimiser.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 128)\n",
    "        self.drop1 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.drop2 = nn.Dropout()\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.drop1(self.fc1(x)))\n",
    "        x = F.relu(self.drop2(self.fc2(x)))\n",
    "        return self.fc3(x)\n",
    "\n",
    "model = MCDropout(Net(), forward=10).to(device)\n",
    "optimiser = torch.optim.Adam(model.parameters())\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can instantiate an acquisition function\n",
    "and an associated `DataManager` instance:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "bald = BALD(model.stochastic_forward, device=device, **data_loader_params)\n",
    "dm = DataManager(bald, X_train, y_train, X_pool, y_pool,\n",
    "                 shuffle=True, **data_loader_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we define the usual training and evaluation loops:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "          dataloader: torchdata.DataLoader,\n",
    "          optimiser: torch.optim.Optimizer,\n",
    "          epochs: int = 50):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    tepochs = tqdm.trange(epochs, file=sys.stdout)\n",
    "    for _ in tepochs:\n",
    "        epoch_losses = []\n",
    "        for x, y in dataloader:\n",
    "            if device:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            epoch_losses.append(loss.item())\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        losses.append(np.mean(epoch_losses))\n",
    "        tepochs.set_postfix(loss=losses[-1])\n",
    "    return losses\n",
    "\n",
    "\n",
    "def evaluate(model: MCDropout,\n",
    "             dataloader: torchdata.DataLoader) -> float:\n",
    "    model.eval()\n",
    "    score = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            if device:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "            _, preds = torch.max(model.predict(x), dim=1)\n",
    "            score += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return score / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can put everything together:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing initial training with 20 points\n",
      "100%|██████████| 5/5 [00:00<00:00, 14.26it/s, loss=0.545]\n",
      "Accuracy = 0.84\n",
      "=====\n",
      "Acquisition iteration 1 (20.00%), training size: 30\n",
      "100%|██████████| 5/5 [00:00<00:00, 22.87it/s, loss=0.389]\n",
      "Accuracy = 0.84\n",
      "=====\n",
      "Acquisition iteration 2 (40.00%), training size: 40\n",
      "100%|██████████| 5/5 [00:00<00:00, 21.68it/s, loss=0.272]\n",
      "Accuracy = 0.88\n",
      "=====\n",
      "Acquisition iteration 3 (60.00%), training size: 50\n",
      "100%|██████████| 5/5 [00:00<00:00, 22.67it/s, loss=0.23] \n",
      "Accuracy = 0.84\n",
      "=====\n",
      "Acquisition iteration 4 (80.00%), training size: 60\n",
      "100%|██████████| 5/5 [00:00<00:00, 23.07it/s, loss=0.284]\n",
      "Accuracy = 0.86\n",
      "=====\n",
      "Acquisition iteration 5 (100.00%), training size: 70\n",
      "100%|██████████| 5/5 [00:00<00:00, 18.30it/s, loss=0.209]\n",
      "Accuracy = 0.9\n",
      "=====\n"
     ]
    },
    {
     "data": {
      "text/plain": "{20: 0.84, 30: 0.84, 40: 0.88, 50: 0.84, 60: 0.86, 70: 0.9}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ITERS = 5\n",
    "EPOCHS = 5\n",
    "\n",
    "# Calculate initial accuracy\n",
    "print(f\"Commencing initial training with {dm.train_size} points\")\n",
    "train(model, dataloader=dm.training_data,\n",
    "      optimiser=optimiser,\n",
    "      epochs=EPOCHS)\n",
    "accs = {dm.train_size: evaluate(model, test_dataset)}\n",
    "print(f\"Accuracy = {accs[dm.train_size]}\\n=====\")\n",
    "\n",
    "# In each iteration, acquire 10 points at once and re-evaluate the model\n",
    "for i in range(ITERS):\n",
    "    dm.acquire(b=10)\n",
    "    print(f\"Acquisition iteration {i + 1} ({(i + 1) / ITERS:.2%}), \"\n",
    "          f\"training size: {dm.train_size}\")\n",
    "    model.reset_weights()\n",
    "    train(model, dataloader=dm.training_data,\n",
    "          optimiser=optimiser, epochs=EPOCHS)\n",
    "    accs[dm.train_size] = evaluate(model, test_dataset)\n",
    "    print(f\"Accuracy = {accs[dm.train_size]}\\n=====\")\n",
    "accs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}