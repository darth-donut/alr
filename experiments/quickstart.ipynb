{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Quick-start\n",
    "\n",
    "This document helps you get up-and-running with `alr` immediately.\n",
    "It should give you a general idea of how to get started with\n",
    "this package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as torchdata\n",
    "import tqdm\n",
    "import sys\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from sklearn.datasets import make_moons\n",
    "from torch import nn\n",
    "\n",
    "from alr import MCDropout\n",
    "from alr.acquisition import BALD\n",
    "from alr.utils import stratified_partition\n",
    "from alr.data import DataManager, UnlabelledDataset\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "data_loader_params = dict(pin_memory=True, num_workers=2, batch_size=32)\n",
    "device = torch.device('gpu:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly, we load and prepare our data.\n",
    "Note that we partitioned the training set into labelled and unlabelled sets\n",
    "using `stratified partition` which balances the number of classes in the training pool:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(20, 250, 1230)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load training data\n",
    "data = make_moons(n_samples=1500)\n",
    "X_train, y_train = torch.as_tensor(data[0], dtype=torch.float32), torch.as_tensor(data[1])\n",
    "X_test, y_test = X_train[-250:], y_train[-250:]\n",
    "X_train, y_train = X_train[:-250], y_train[:-250]\n",
    "\n",
    "# convert into Datasets/DataLoaders\n",
    "\n",
    "# 1. partition data using stratified_partition\n",
    "X_train, y_train, X_pool, y_pool = stratified_partition(X_train, y_train, train_size=20)\n",
    "\n",
    "# 2. create Datasets/DataLoader objects\n",
    "train = torchdata.DataLoader(torchdata.TensorDataset(X_train, y_train))\n",
    "test = torchdata.DataLoader(torchdata.TensorDataset(X_test, y_test), **data_loader_params)\n",
    "pool = UnlabelledDataset(torchdata.TensorDataset(X_pool, y_pool))\n",
    "\n",
    "len(train.dataset), len(test.dataset), len(pool)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`MCDropout` lets us define a Bayesian NN. It provides an implementation\n",
    "for `stochastic_forward` which we will use in the next section for the\n",
    "acquisition function.\n",
    "\n",
    "> Notice the dropout layers have been changed to their `Persistent` versions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "MCDropout(\n  (base_model): Net(\n    (fc1): Linear(in_features=2, out_features=128, bias=True)\n    (drop1): PersistentDropout(p=0.5, inplace=False)\n    (fc2): Linear(in_features=128, out_features=256, bias=True)\n    (drop2): PersistentDropout(p=0.5, inplace=False)\n    (fc3): Linear(in_features=256, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a regular model and an optimiser.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 128)\n",
    "        self.drop1 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.drop2 = nn.Dropout()\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.drop1(self.fc1(x)))\n",
    "        x = F.relu(self.drop2(self.fc2(x)))\n",
    "        return self.fc3(x)\n",
    "\n",
    "model = MCDropout(Net(), forward=10).to(device)\n",
    "optimiser = torch.optim.Adam(model.parameters())\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can instantiate an acquisition function\n",
    "and an associated `DataManager` instance:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "bald = BALD(model.stochastic_forward, device=device, **data_loader_params)\n",
    "dm = DataManager(train.dataset, pool, bald)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we define the usual training and evaluation loops:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "          dataloader: torchdata.DataLoader,\n",
    "          optimiser: torch.optim.Optimizer,\n",
    "          epochs: int = 50):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    tepochs = tqdm.trange(epochs, file=sys.stdout)\n",
    "    for _ in tepochs:\n",
    "        epoch_losses = []\n",
    "        for x, y in dataloader:\n",
    "            if device:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            epoch_losses.append(loss.item())\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        losses.append(np.mean(epoch_losses))\n",
    "        tepochs.set_postfix(loss=losses[-1])\n",
    "    return losses\n",
    "\n",
    "\n",
    "def evaluate(model: MCDropout,\n",
    "             dataloader: torchdata.DataLoader) -> float:\n",
    "    model.eval()\n",
    "    score = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            if device:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "            _, preds = torch.max(model.predict(x), dim=1)\n",
    "            score += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return score / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can put everything together:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquisition iteration 1 (20.00%), training size: 20\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.66it/s, loss=0.695]\n",
      "Accuracy = 0.576\n",
      "=====\n",
      "Acquisition iteration 2 (40.00%), training size: 30\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.59it/s, loss=0.587]\n",
      "Accuracy = 0.644\n",
      "=====\n",
      "Acquisition iteration 3 (60.00%), training size: 40\n",
      "100%|██████████| 1/1 [00:00<00:00, 26.26it/s, loss=0.513]\n",
      "Accuracy = 0.672\n",
      "=====\n",
      "Acquisition iteration 4 (80.00%), training size: 50\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.05it/s, loss=0.367]\n",
      "Accuracy = 0.692\n",
      "=====\n",
      "Acquisition iteration 5 (100.00%), training size: 60\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.78it/s, loss=0.297]\n",
      "Accuracy = 0.668\n",
      "=====\n"
     ]
    },
    {
     "data": {
      "text/plain": "{20: 0.576, 30: 0.644, 40: 0.672, 50: 0.692, 60: 0.668}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ITERS = 5\n",
    "EPOCHS = 1\n",
    "accs = {}\n",
    "\n",
    "# In each iteration, acquire 10 points at once and re-evaluate the model\n",
    "for i in range(ITERS):\n",
    "    print(f\"Acquisition iteration {i + 1} ({(i + 1) / ITERS:.2%}), \"\n",
    "          f\"training size: {dm.n_labelled}\")\n",
    "    model.reset_weights()\n",
    "    train(model, dataloader=torchdata.DataLoader(dm.labelled, **data_loader_params),\n",
    "          optimiser=optimiser, epochs=EPOCHS)\n",
    "    accs[dm.n_labelled] = evaluate(model, test)\n",
    "    print(f\"Accuracy = {accs[dm.n_labelled]}\\n=====\")\n",
    "    dm.acquire(b=10)\n",
    "accs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}