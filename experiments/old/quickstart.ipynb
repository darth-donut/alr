{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQm2CBEwH0Bj"
   },
   "source": [
    "# Quick-start\n",
    "\n",
    "This document helps you get up-and-running with `alr` immediately.\n",
    "It should give you a general idea of how to get started with\n",
    "this package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:31:08.179056Z",
     "start_time": "2020-05-03T22:31:06.734269Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NrKAk6czH0Bk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "from alr import MCDropout\n",
    "from alr.acquisition import RandomAcquisition, BALD\n",
    "from alr.utils import stratified_partition, eval_fwd_exp\n",
    "from alr.data import DataManager, UnlabelledDataset\n",
    "from alr.data.datasets import Dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "data_loader_params = dict(pin_memory=True, num_workers=2, batch_size=32)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4Wo9ZJCH0Bn"
   },
   "source": [
    "Firstly, we load and prepare our data.\n",
    "Note that we partitioned the training set into labelled and unlabelled sets\n",
    "using `stratified partition` which balances the number of classes in the training pool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:31:08.389980Z",
     "start_time": "2020-05-03T22:31:08.199811Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6WFbVtkPH0Bo",
    "outputId": "3e54abca-4483-4eae-d30f-06ba7f385b2f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10000, 59980)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load training data\n",
    "train, test = Dataset.MNIST.get()\n",
    "train, pool = stratified_partition(train, Dataset.MNIST.about.n_class, size=20)\n",
    "pool = UnlabelledDataset(pool)\n",
    "len(train), len(test), len(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7KniNLYH0Bu"
   },
   "source": [
    "`MCDropout` lets us define a Bayesian NN. It provides an implementation\n",
    "for `stochastic_forward` which we will use in the next section for the\n",
    "acquisition function.\n",
    "\n",
    "> Notice the dropout layers have been changed to their `Persistent` versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:31:08.472113Z",
     "start_time": "2020-05-03T22:31:08.404272Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "wMbtwZj0H0Bu",
    "outputId": "e069e054-6b57-4497-86db-93c02bf27a9a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCDropout(\n",
       "  (base_model): Net(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (dropout1): PersistentDropout2d(p=0.5, inplace=False)\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (dropout2): PersistentDropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (dropout3): PersistentDropout(p=0.5, inplace=False)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a regular model and an optimiser.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        # 32 24 24\n",
    "        self.dropout1 = nn.Dropout2d()\n",
    "        # maxpool --\n",
    "        # 32 12 12\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        # 64 8 8\n",
    "        self.dropout2 = nn.Dropout2d()\n",
    "        # maxpool --\n",
    "        # 64 4 4\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.dropout3 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.dropout1(F.relu(self.conv1(x))), 2)\n",
    "        x = F.max_pool2d(self.dropout2(F.relu(self.conv2(x))), 2)\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = self.fc2(self.dropout3(F.relu(self.fc1(x))))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = MCDropout(Net(), forward=20).to(device)\n",
    "model.compile(criterion=torch.nn.NLLLoss(),\n",
    "              optimiser=torch.optim.Adam(model.parameters()))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QRevqvqKH0Bx"
   },
   "source": [
    "Now, we can instantiate an acquisition function\n",
    "and an associated `DataManager` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:31:08.480779Z",
     "start_time": "2020-05-03T22:31:08.478322Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "h8Iiyc4eH0By",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bald = BALD(\n",
    "    eval_fwd_exp(model), device=device,\n",
    "    batch_size=1024, pin_memory=True, num_workers=2\n",
    ")\n",
    "dm = DataManager(train, pool, bald)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ABdzZSiCH0B0"
   },
   "source": [
    "Finally, the vanilla acquisition loop looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:34:05.239418Z",
     "start_time": "2020-05-03T22:31:08.482985Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EpjKPQ9FH0B1",
    "outputId": "b3701105-6a51-4c27-e880-ad8e00514b0d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Iteration 1 (4.17%), training size: 20 ====\n",
      "train_acc = 1.00, train_loss = 0.24, test_acc = 0.63, test_loss = 1.88\n",
      "==== Iteration 2 (8.33%), training size: 30 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 1.00, train_loss = 0.12, test_acc = 0.67, test_loss = 1.60\n",
      "==== Iteration 3 (12.50%), training size: 40 ====\n",
      "train_acc = 1.00, train_loss = 0.09, test_acc = 0.74, test_loss = 1.33\n",
      "==== Iteration 4 (16.67%), training size: 50 ====\n",
      "train_acc = 1.00, train_loss = 0.47, test_acc = 0.79, test_loss = 1.09\n",
      "==== Iteration 5 (20.83%), training size: 60 ====\n",
      "train_acc = 1.00, train_loss = 0.34, test_acc = 0.80, test_loss = 1.04\n",
      "==== Iteration 6 (25.00%), training size: 70 ====\n",
      "train_acc = 1.00, train_loss = 0.18, test_acc = 0.77, test_loss = 1.09\n",
      "==== Iteration 7 (29.17%), training size: 80 ====\n",
      "train_acc = 1.00, train_loss = 0.38, test_acc = 0.80, test_loss = 1.01\n",
      "==== Iteration 8 (33.33%), training size: 90 ====\n",
      "train_acc = 1.00, train_loss = 0.34, test_acc = 0.83, test_loss = 0.89\n",
      "==== Iteration 9 (37.50%), training size: 100 ====\n",
      "train_acc = 0.99, train_loss = 0.70, test_acc = 0.81, test_loss = 1.00\n",
      "==== Iteration 10 (41.67%), training size: 110 ====\n",
      "train_acc = 1.00, train_loss = 0.40, test_acc = 0.85, test_loss = 0.80\n",
      "==== Iteration 11 (45.83%), training size: 120 ====\n",
      "train_acc = 1.00, train_loss = 0.31, test_acc = 0.88, test_loss = 0.73\n",
      "==== Iteration 12 (50.00%), training size: 130 ====\n",
      "train_acc = 0.97, train_loss = 0.77, test_acc = 0.87, test_loss = 0.90\n",
      "==== Iteration 13 (54.17%), training size: 140 ====\n",
      "train_acc = 0.99, train_loss = 0.24, test_acc = 0.90, test_loss = 0.65\n",
      "==== Iteration 14 (58.33%), training size: 150 ====\n",
      "train_acc = 1.00, train_loss = 0.21, test_acc = 0.92, test_loss = 0.54\n",
      "==== Iteration 15 (62.50%), training size: 160 ====\n",
      "train_acc = 1.00, train_loss = 0.26, test_acc = 0.93, test_loss = 0.51\n",
      "==== Iteration 16 (66.67%), training size: 170 ====\n",
      "train_acc = 0.99, train_loss = 0.44, test_acc = 0.93, test_loss = 0.55\n",
      "==== Iteration 17 (70.83%), training size: 180 ====\n",
      "train_acc = 0.99, train_loss = 0.27, test_acc = 0.92, test_loss = 0.54\n",
      "==== Iteration 18 (75.00%), training size: 190 ====\n",
      "train_acc = 1.00, train_loss = 0.26, test_acc = 0.93, test_loss = 0.50\n",
      "==== Iteration 19 (79.17%), training size: 200 ====\n",
      "train_acc = 1.00, train_loss = 0.22, test_acc = 0.93, test_loss = 0.50\n",
      "==== Iteration 20 (83.33%), training size: 210 ====\n",
      "train_acc = 1.00, train_loss = 0.20, test_acc = 0.93, test_loss = 0.48\n",
      "==== Iteration 21 (87.50%), training size: 220 ====\n",
      "train_acc = 1.00, train_loss = 0.25, test_acc = 0.93, test_loss = 0.46\n",
      "==== Iteration 22 (91.67%), training size: 230 ====\n",
      "train_acc = 1.00, train_loss = 0.34, test_acc = 0.94, test_loss = 0.50\n",
      "==== Iteration 23 (95.83%), training size: 240 ====\n",
      "train_acc = 1.00, train_loss = 0.25, test_acc = 0.94, test_loss = 0.46\n",
      "==== Iteration 24 (100.00%), training size: 250 ====\n",
      "train_acc = 1.00, train_loss = 0.25, test_acc = 0.95, test_loss = 0.42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{20: 0.6266,\n",
       " 30: 0.6722,\n",
       " 40: 0.7433,\n",
       " 50: 0.7899,\n",
       " 60: 0.7966,\n",
       " 70: 0.7696,\n",
       " 80: 0.8032,\n",
       " 90: 0.835,\n",
       " 100: 0.8087,\n",
       " 110: 0.8538,\n",
       " 120: 0.8807,\n",
       " 130: 0.8692,\n",
       " 140: 0.8975,\n",
       " 150: 0.9246,\n",
       " 160: 0.9333,\n",
       " 170: 0.9326,\n",
       " 180: 0.9161,\n",
       " 190: 0.9301,\n",
       " 200: 0.934,\n",
       " 210: 0.9295,\n",
       " 220: 0.9349,\n",
       " 230: 0.9354,\n",
       " 240: 0.9411,\n",
       " 250: 0.9501}"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ITERS = 24\n",
    "EPOCHS = 50\n",
    "accs = {}\n",
    "\n",
    "# In each iteration, acquire `b` points\n",
    "for i in range(ITERS):\n",
    "    print(f\"==== Iteration {i + 1} ({(i + 1) / ITERS:.2%}), \"\n",
    "          f\"training size: {dm.n_labelled} ====\")\n",
    "    # reset weights to original values when the model was first created\n",
    "    model.reset_weights()\n",
    "    # fit = train\n",
    "    result = model.fit(\n",
    "        train_loader=torchdata.DataLoader(\n",
    "            dm.labelled, shuffle=True, **data_loader_params\n",
    "        ),\n",
    "        train_acc=True, epochs=EPOCHS, device=device, quiet=True\n",
    "    )\n",
    "    # evaluate the model to obtain its test accuracy\n",
    "    test_acc, test_loss = model.evaluate(\n",
    "        data=torchdata.DataLoader(test, **data_loader_params),\n",
    "        device=device, quiet=True\n",
    "    )\n",
    "    # display results\n",
    "    result.reduce('last', inplace=True)\n",
    "    accs[dm.n_labelled] = test_acc\n",
    "    print(f\"train_acc = {result.train_acc:.2f}, \"\n",
    "          f\"train_loss = {result.train_loss:.2f}, \"\n",
    "          f\"test_acc = {test_acc:.2f}, \"\n",
    "          f\"test_loss = {test_loss:.2f}\")\n",
    "    # acquire `b` points from unlabelled pool\n",
    "    dm.acquire(b=10)\n",
    "accs\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "quickstart.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
